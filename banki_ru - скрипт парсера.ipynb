{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNjdqqe5Esko",
        "outputId": "10decfe3-31d0-4f9f-a751-35d83be8196f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fake_useragent\n",
            "  Downloading fake_useragent-1.5.1-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: fake_useragent\n",
            "Successfully installed fake_useragent-1.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install fake_useragent"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import csv\n",
        "import time\n",
        "import json\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import concurrent.futures\n",
        "from bs4 import BeautifulSoup\n",
        "from fake_useragent import UserAgent\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed"
      ],
      "metadata": {
        "id": "hmoJD-_TOmCJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/Ledka17/Parsing_banki_ru/blob/master/Parsing_banki_ru.ipynb\n",
        "# https://github.com/kotikkonstantin/bankiru/blob/master/bankiru_parsing.ipynb\n",
        "# https://github.com/KKQUEN/banki.ru_reviews_parser/blob/main/parser_banki.ru_ModeratedReviews.ipynb\n",
        "\n",
        "# про многопоточность\n",
        "# https://habr.com/ru/companies/otus/articles/771346/"
      ],
      "metadata": {
        "id": "qJkVE1Ux40Pg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сначала парсим только ссылки, потом парсим текст, гуляя по ссылкам.\n",
        "\n",
        "Парсим ссылки - ищем вручную нужный диапазон страниц и парсим. будут дубликаты, так как отзывы добавляются очень быстро. так что парсим ссылки, открываем док и убираем дубликаты.\n",
        "Затем парсим по ссылкам инфу."
      ],
      "metadata": {
        "id": "_xRsQKLhKPCE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# список банков - DONE!\n",
        "\n",
        "%%time\n",
        "\n",
        "for year in [2023, 2024]:\n",
        "\n",
        "  df = pd.DataFrame({'place':[], 'name':[], 'rating':[], 'responses':[], 'answers':[]})\n",
        "\n",
        "  page = 1\n",
        "  i = 1\n",
        "  while True:\n",
        "\n",
        "    url = f'https://www.banki.ru/services/responses/?date={year}&page={page}'\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    items = soup.find_all(\"script\", {\"type\": \"application/ld+json\"})\n",
        "\n",
        "    if len(items) == 1:\n",
        "      break\n",
        "\n",
        "    for item in items[:50]:\n",
        "      data = json.loads(item.text)\n",
        "      if 'name' in data:\n",
        "        place = i\n",
        "        name = data['name']\n",
        "        rating = data['aggregateRating']['ratingValue']\n",
        "        responses = data['aggregateRating']['ratingCount']\n",
        "        answers = data['aggregateRating']['reviewCount']\n",
        "\n",
        "        df.loc[len(df)] = [i, name, rating, responses, answers]\n",
        "        i += 1\n",
        "    page += 1\n",
        "\n",
        "  df.to_csv(f'banki_banks_{year}.csv', index=False)"
      ],
      "metadata": {
        "id": "JFG4mIfbwOP5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ссылки на отзывы - DONE! - однопотоковый\n",
        "\n",
        "%%time\n",
        "\n",
        "base_url = 'https://www.banki.ru'\n",
        "\n",
        "with open('banki_responses_urls.csv', \"w\", encoding='utf-8') as w_file:\n",
        "  file_writer = csv.writer(w_file, delimiter = \",\", lineterminator=\"\\n\")\n",
        "  file_writer.writerow(['url'])\n",
        "\n",
        "  for page in tqdm(range(1,5)):\n",
        "\n",
        "    url = f'https://www.banki.ru/services/responses/list/?type=all&page={page}'\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "\n",
        "    items = soup.find_all('a', attrs={'class': 'link-simple',\n",
        "                                      'data-gtm-click': '{\"event\":\"GTM_event\",\"eventCategory\":\"ux_data\",\"eventAction\":\"click_responses_response_user_rating_banking_allReviewsPage\"}'})\n",
        "\n",
        "    for item in items:\n",
        "\n",
        "      url_resp = base_url+item['href']\n",
        "\n",
        "      file_writer.writerow([url_resp])\n",
        "\n",
        "    if page % 10 == 0:\n",
        "      w_file.flush()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sko62LAeZwOs",
        "outputId": "88ebb528-69b9-440a-eca4-1e661e4919ae"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [00:07<00:00,  1.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1.2 s, sys: 33.6 ms, total: 1.24 s\n",
            "Wall time: 7.04 s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ссылки на отзывы - DONE! (многопоточный)\n",
        "\n",
        "%%time\n",
        "\n",
        "ua = UserAgent()\n",
        "headers = {'user-Agent': ua.google}\n",
        "\n",
        "\n",
        "def get_urls(url):\n",
        "\n",
        "  response = requests.get(url, headers=headers)\n",
        "  soup = BeautifulSoup(response.content, 'html.parser')\n",
        "  # print(response.request.headers['User-Agent'], response.status_code)\n",
        "\n",
        "  items = soup.find_all('a', attrs={'class': 'link-simple',\n",
        "                                    'data-gtm-click': '{\"event\":\"GTM_event\",\"eventCategory\":\"ux_data\",\"eventAction\":\"click_responses_response_user_rating_banking_allReviewsPage\"}'})\n",
        "\n",
        "  urls = []\n",
        "  for item in items:\n",
        "    urls.append('https://www.banki.ru' + item['href'])\n",
        "\n",
        "  return urls\n",
        "\n",
        "\n",
        "def write_to_csv(urls):\n",
        "  with open('banki_urls.csv', \"a\", encoding='utf-8') as w_file:\n",
        "    file_writer = csv.writer(w_file, delimiter=\",\", lineterminator=\"\\n\")\n",
        "    for url in urls:\n",
        "      file_writer.writerow([url])\n",
        "\n",
        "\n",
        "urls = [f'https://www.banki.ru/services/responses/list/?type=all&page={page}' for page in range(1, 5)]\n",
        "\n",
        "with open('banki_urls.csv', \"a\", encoding='utf-8') as w_file:\n",
        "    file_writer = csv.writer(w_file, delimiter=\",\", lineterminator=\"\\n\")\n",
        "    file_writer.writerow(['url'])\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "  futures = [executor.submit(get_urls, url) for url in urls]\n",
        "  for future in tqdm(as_completed(futures)):\n",
        "    results = future.result()\n",
        "    write_to_csv(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZeC9ZSHgyDLy",
        "outputId": "c7c4f730-9f05-47ec-918d-2ef0fcff0867"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "4it [00:02,  1.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1.27 s, sys: 35.2 ms, total: 1.31 s\n",
            "Wall time: 2.47 s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# отзывы - DONE (многопоточность)\n",
        "\n",
        "ua = UserAgent()\n",
        "headers = {'user-Agent': ua.chrome}\n",
        "\n",
        "def get_review(url):\n",
        "\n",
        "  response = requests.get(url, headers=headers)\n",
        "  # print(url, response.request.headers['User-Agent'], response.status_code)\n",
        "  soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "  date_review = soup.find('span', {'class':'l10fac986'}).text[4:14]\n",
        "  time_review = soup.find('span', {'class':'l10fac986'}).text[15:20]\n",
        "  user_name = soup.find('span', {'class':'l17191939'}).text.strip()\n",
        "  user_city = soup.find('span', {'class':'l3a372298'}).text\n",
        "  review_title = soup.find('h1', {'class':'text-header-0 le856f50c'}).text.strip()\n",
        "  review_text = soup.find(\"div\", {\"class\":\"lf4cbd87d ld6d46e58 lfd76152f\"}).text.strip()\n",
        "  rating = soup.find('div', {'class':'lbb810226'}).text.strip()\n",
        "  if rating == 'Без оценки':\n",
        "    review_status = 'unk'\n",
        "  else:\n",
        "    review_status = soup.find('section', {'class':'lf4cbd87d l9656ec89 lfd76152f'}).text.strip()\n",
        "\n",
        "  additional_grades = {}\n",
        "  for txts6 in soup.find_all('div', {\"class\": 'text-size-6'}):\n",
        "      additional_grades[txts6.text] = 0\n",
        "  i = 0\n",
        "  for grade in soup.find_all('div', {'class': 'ld017b199'}):\n",
        "      current_key = list(additional_grades.keys())[i]\n",
        "      additional_grades[current_key] = str(grade).count(\"l61f54b7b\")\n",
        "      i += 1\n",
        "  clear_conditions_rating = additional_grades.get('Прозрачные условия', 0)\n",
        "  polite_staff_rating = additional_grades.get('Вежливые сотрудники', 0)\n",
        "  support_rating = additional_grades.get('Доступность и поддержка', 0)\n",
        "  app_site_rating = additional_grades.get('Удобство приложения, сайта', 0)\n",
        "\n",
        "  bank_name = soup.find('img', {'class':'lazy-load'})['alt']\n",
        "  bank_ans = soup.find('div', {'class':'l0e7bcaa7'})\n",
        "  if bank_ans is None:\n",
        "    is_bank_ans = 'no'\n",
        "    date_bank_ans = 'unk'\n",
        "    time_bank_ans = 'unk'\n",
        "    bank_text_ans = 'unk'\n",
        "  else:\n",
        "    is_bank_ans = 'yes'\n",
        "    date_bank_ans = soup.find('div', {'class':'l0e7bcaa7'}).find('div', {'class':'l46c44745'}).text[:10].strip()\n",
        "    time_bank_ans = soup.find('div', {'class':'l0e7bcaa7'}).find('div', {'class':'l46c44745'}).text[11:19].strip()\n",
        "    bank_text_ans = soup.find('div', {'class':'l0e7bcaa7'}).find('div', {'class':'lb1789875'}).text.strip()\n",
        "\n",
        "  review_row = [url, date_review, time_review, user_name, user_city, review_title, review_text, review_status,\n",
        "               rating, clear_conditions_rating, polite_staff_rating, support_rating, app_site_rating,\n",
        "               bank_name, is_bank_ans, time_bank_ans, date_bank_ans, bank_text_ans]\n",
        "\n",
        "  return review_row\n",
        "\n",
        "\n",
        "def write_to_csv(review_row):\n",
        "  with open('banki_reviews.csv', \"a\", encoding='utf-8') as w_file:\n",
        "    file_writer = csv.writer(w_file, delimiter=\",\", lineterminator=\"\\n\")\n",
        "    file_writer.writerow(review_row)\n",
        "\n",
        "urls = pd.read_csv('banki_urls.csv')\n",
        "urls = urls.iloc[:,0].tolist()\n",
        "\n",
        "with open('banki_reviews.csv', \"w\", encoding='utf-8') as w_file:\n",
        "  file_writer = csv.writer(w_file, delimiter = \",\", lineterminator=\"\\n\")\n",
        "  file_writer.writerow(['url', 'date_review', 'time_review', 'user_name', 'user_city', 'review_title', 'review_text', 'review_status',\n",
        "                        'rating', 'clear_conditions_rating', 'polite_staff_rating', 'support_rating', 'app_site_rating',\n",
        "                        'bank_name', 'is_bank_ans', 'time_bank_ans', 'date_bank_ans', 'bank_text_ans'])\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "  futures = [executor.submit(get_review, url) for url in urls]\n",
        "  for future in tqdm(as_completed(futures)):\n",
        "    results = future.result()\n",
        "    write_to_csv(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UI6FFFr62Vam",
        "outputId": "2dfc25a8-a6cb-494e-cad3-acde50b119a7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100it [00:43,  2.28it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aHSqujO7fIeC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}