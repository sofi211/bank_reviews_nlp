{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XNjdqqe5Esko"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import time\n",
        "import json\n",
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/Ledka17/Parsing_banki_ru/blob/master/Parsing_banki_ru.ipynb\n",
        "# https://github.com/kotikkonstantin/bankiru/blob/master/bankiru_parsing.ipynb\n",
        "# https://github.com/KKQUEN/banki.ru_reviews_parser/blob/main/parser_banki.ru_ModeratedReviews.ipynb"
      ],
      "metadata": {
        "id": "qJkVE1Ux40Pg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# список банков\n",
        "\n",
        "for year in [2023, 2024]:\n",
        "\n",
        "  df = pd.DataFrame({'place':[], 'name':[], 'rating':[], 'responses':[], 'answers':[]})\n",
        "\n",
        "  page = 1\n",
        "  i = 1\n",
        "  while True:\n",
        "\n",
        "    url = f'https://www.banki.ru/services/responses/?date={year}&page={page}'\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    items = soup.find_all(\"script\", {\"type\": \"application/ld+json\"})\n",
        "\n",
        "    if len(items) == 1:\n",
        "      break\n",
        "\n",
        "    for item in items[:50]:\n",
        "      data = json.loads(item.text)\n",
        "      if 'name' in data:\n",
        "        place = i\n",
        "        name = data['name']\n",
        "        rating = data['aggregateRating']['ratingValue']\n",
        "        responses = data['aggregateRating']['ratingCount']\n",
        "        answers = data['aggregateRating']['reviewCount']\n",
        "\n",
        "        df.loc[len(df)] = [i, name, rating, responses, answers]\n",
        "        i += 1\n",
        "    page += 1\n",
        "\n",
        "  df.to_csv(f'banki_banks_{year}.csv', index=False)"
      ],
      "metadata": {
        "id": "JFG4mIfbwOP5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# отзывы\n",
        "\n",
        "base_url = 'https://www.banki.ru'\n",
        "page = 1\n",
        "\n",
        "with open('banki_responses.csv', \"w\", encoding='utf-8') as w_file:\n",
        "  file_writer = csv.writer(w_file, delimiter = \",\", lineterminator=\"\\n\")\n",
        "  file_writer.writerow(['url', 'date_response', 'time_response', 'bank_name', 'username', 'user_city', 'review_title', 'review_text'])\n",
        "\n",
        "  while True:\n",
        "\n",
        "    url = f'https://www.banki.ru/services/responses/list/?page={page}&type=all'\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    items = soup.find_all('a', attrs={'class': 'link-simple',\n",
        "                                      'data-gtm-click': '{\"event\":\"GTM_event\",\"eventCategory\":\"ux_data\",\"eventAction\":\"click_responses_response_user_rating_banks\"}'})\n",
        "    for item in items:\n",
        "\n",
        "      url_resp = base_url+item['href']\n",
        "\n",
        "      response_resp = requests.get(url_resp)\n",
        "      soup_resp = BeautifulSoup(response_resp.content, 'html.parser')\n",
        "\n",
        "      date_time = soup_resp.find('span', {'class':'l10fac986'}).text\n",
        "      date = date_time[4:14]\n",
        "      time = date_time[15:]\n",
        "      bank_name = soup_resp.find('img', {'class':'lazy-load'})['alt']\n",
        "      review_title = soup_resp.find('h1', {'class':'text-header-0 le856f50c'}).text\n",
        "      review_text = soup_resp.find(\"div\", {\"class\":\"lf4cbd87d ld6d46e58 lfd76152f\"}).text\n",
        "      username = soup_resp.find('span', {'class':'l17191939'}).text\n",
        "      user_city = soup_resp.find('span', {'class':'l3a372298'}).text\n",
        "\n",
        "\n",
        "      file_writer.writerow([url_resp, date, time, bank_name, username, user_city, review_title, review_text])\n",
        "\n",
        "\n",
        "    w_file.flush()\n",
        "    page += 1\n",
        "\n",
        "    if page == 3:   # тут потом переписать - ограничение по дате!\n",
        "      break"
      ],
      "metadata": {
        "id": "aRLViYiT5BEl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XNa4l25ATzFv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}